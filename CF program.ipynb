{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74093171",
   "metadata": {},
   "source": [
    "KNN Implementation description:\n",
    "Initially I split the Implementation into multiple functions, these are: dataSplit, getDistances, labelSelection, applyToTestSet, and predictionAccuracy.\n",
    "These functions are called by the user when required to progress the application of a KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221ef5e",
   "metadata": {},
   "source": [
    "Imports for the functions include numpy and math, numpy is used to define the arrays and infinty.\n",
    "Math is used to floor a value and to square root a value.\n",
    "Both of these uses could have been implemented without imports however this would probabily reduce efficiency and was not specified as an additional task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac476f3",
   "metadata": {},
   "source": [
    "Notes on my implementation:\n",
    "I made several functions that allow a user to split data to training and test data, create a KNN (Variable size of K) and obtain a conformity measure, P-value and False-P-Value. I applied this to the Iris dataset and ionosphere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5de47b",
   "metadata": {},
   "source": [
    "Structure: Imports -> KNN functions -> Conformal Predictor functions -> Tests on Iris dataset -> Tests on Ionosphere dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff206f38",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f48d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8eb3e5",
   "metadata": {},
   "source": [
    "# KNN functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9846999",
   "metadata": {},
   "source": [
    "The first function: dataSplit, has the same purpose as train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bc4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the data split function\n",
    "#inputs: data = the input data\n",
    "#labels = labels associated with the input data\n",
    "#ratio = % to go to testSet\n",
    "#shuffleVar = the seed to set random to\n",
    "#outputs: array of: trainingSet, testSet, trainingLabels, testLabels\n",
    "def dataSplit(data,labels,ratio,shuffleVar):\n",
    "    #take input data and ratio in order to find the size of the test and training datasets\n",
    "    testSize = math.floor(ratio*data.shape[0])\n",
    "    trainSize = data.shape[0]-testSize\n",
    "    \n",
    "    #set the random seed, then shuffle an array of indexs to move around the order of the input data\n",
    "    np.random.seed(shuffleVar)\n",
    "    indexs = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indexs)\n",
    "    #create 2 new variables to store the new data in with its new order\n",
    "    dataSet = np.zeros(data.shape)\n",
    "    labelSet = np.zeros(data.shape[0])\n",
    "    #loop through all data applying the new index\n",
    "    for i in range(trainSize+testSize):\n",
    "        dataSet[i] = data[indexs[i]]\n",
    "        labelSet[i] = labels[indexs[i]]\n",
    "    data=dataSet\n",
    "    labels=labelSet\n",
    "    #create arrays to store the separated labels and data \n",
    "    trainingSet = np.zeros((trainSize,data.shape[1]))\n",
    "    testSet = np.zeros((testSize,data.shape[1]))\n",
    "    trainingLabels = np.zeros(trainSize)\n",
    "    testLabels = np.zeros(testSize)\n",
    "    #loop through and split the data\n",
    "    for i in range(trainSize+testSize):\n",
    "        if i<testSize:\n",
    "            testSet[i] = data[i]\n",
    "            testLabels[i] = labels[i]\n",
    "        else:\n",
    "            trainingSet[i-testSize] = data[i]\n",
    "            trainingLabels[i-testSize] = labels[i]\n",
    "    #output the array of: trainingSet, testSet, trainingLabels, testLabels\n",
    "    return [trainingSet,testSet,trainingLabels,testLabels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8229ecb",
   "metadata": {},
   "source": [
    "The second function getDistances takes the input of a test value, the training set and the number k of nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5894915",
   "metadata": {},
   "source": [
    "Note on the computational complexity. This function is where the majority of the computation will occur, this being the case it is where computational complexity is a primary concern. There are multiple nested for loops within this function, the first 2 for loops have a complexity of \"size of the training dataset\"*\"number of dimensions the data has\" The second set of for loops has \"size of the training dataset\"*k. As k and \"number of dimensions the data has\" are relitively small complexity can be sumerised by: O(\"size of the training dataset\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d867e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the distance funtion\n",
    "#x is the test value and y are the training values, k=K\n",
    "#return the smallest distance and its index in y\n",
    "#Computational complexity: this uses 2 for loops going through all of the training dataset and then calculates the distance based on the number of dimensions the dataset has. This function will do the majority of the computational work\n",
    "#There are ways to reduce computational complexity with memory however this introduce added complexity into the code\n",
    "def getDistances(x,y,k):\n",
    "    distances = np.zeros((y.shape[0],2))\n",
    "    #get the distance from each of the points in y to the point x\n",
    "    for j in range(y.shape[0]):\n",
    "        #calculate the distance based on the number of dimensions (e.g. iris has 4 so the loop will run 4 times)\n",
    "        for i in range(x.shape[0]):\n",
    "            #find the square of each differnce\n",
    "            distances[j][0] += (x[i]-y[j][i])**2\n",
    "        #root the value to find the distance\n",
    "        distances[j] = np.array([math.sqrt(distances[j][0]),j])\n",
    "    #define arrays minimumDistance (the output), and tempDistance (used to store a value temparerily)\n",
    "    minimumDistance = np.full((k,2),np.inf)\n",
    "    tempDistance = np.full((2),0)\n",
    "    #loop through all of the distances\n",
    "    for i in range(distances.shape[0]):\n",
    "        #if any of the distances are lower than the largest distance stored allow the distances stored to be updated\n",
    "        if distances[i][0]<minimumDistance[k-1][0]:\n",
    "            #loop through all minimum distances stored\n",
    "            for j in range(k):\n",
    "                #if the new duistance is smaller than the smallest stored replace. Use a tempary array to enable switching\n",
    "                if distances[i][0]<minimumDistance[j][0]:\n",
    "                    tempDistance = np.ndarray.copy(minimumDistance[j])\n",
    "                    minimumDistance[j] = distances[i]\n",
    "                    if tempDistance[0] ==np.inf:\n",
    "                        j=k\n",
    "                    else:\n",
    "                        distances[i][0] = tempDistance[0]\n",
    "                        distances[i][1] = tempDistance[1]\n",
    "    \n",
    "    \n",
    "    return minimumDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c549fac",
   "metadata": {},
   "source": [
    "The third function labelSelection takes the output from the previous function and assigns it a label based upon a voting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5604ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the label selection function\n",
    "#inputs are:\n",
    "#x=output from getDistances\n",
    "#y=the example datasetlabels\n",
    "#k=k nearest neighbors\n",
    "def labelSelection(x, y, k):\n",
    "    #create array to hold votes\n",
    "    labelSet = np.zeros(np.unique(y).shape[0])\n",
    "    #loop through all labels which were close to \n",
    "    for i in range(k):\n",
    "        #get index of the distance from x to get the relevent label from y to add one to label set\n",
    "        labelSet[np.where(np.unique(y)== y[int(x[i][1])])] += 1\n",
    "    #return the value found in array of all unique labels in position voted on\n",
    "    return np.unique(y)[np.where(labelSet == np.amax(labelSet))[0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95ee6b",
   "metadata": {},
   "source": [
    "This function combines the previous 2 functions over an entire dataset, the rational behind separating the functions is that the user may sometimes wish to analyse a single datapoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e5dc4",
   "metadata": {},
   "source": [
    "Note on complexity: This function calls getDistances within another for loop. This increases the computational complexity significantly to O(\"size of the training dataset\"*\"size of the test dataset\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2867a438",
   "metadata": {},
   "source": [
    "As \"size of the training dataset\" = data * m, and \"size of the test dataset\" = data * (1-m). big O can be simplified (at the cost of trueness) to O(Data ** 2), where data is the size of the dataset before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868c3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define applyToTestSet function\n",
    "#inputs: x=trainingSet, y=traningLabels, z=testSet, k=K\n",
    "def applyToTestSet(x,y,z,k):\n",
    "    #create an array to store the output list of predicted labels\n",
    "    predictedLabels = np.zeros(z.shape[0])\n",
    "    #loop through all of the test datapoints and obtain a label prediction\n",
    "    for i in range(z.shape[0]):\n",
    "        distancesForZi = getDistances(z[i], x, k)\n",
    "        predictedLabels[i] = labelSelection(distancesForZi,y,k)\n",
    "    return predictedLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe71305",
   "metadata": {},
   "source": [
    "The final function defines the accuracy: It loops through all of the predicted labels and compares them to their true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ff3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy function\n",
    "#inputs x=the predictd labels and y=the test labels\n",
    "def predictionAccuracy(x,y):\n",
    "    numberCorrect = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        if x[i]==y[i]:\n",
    "            numberCorrect +=1\n",
    "    return numberCorrect/x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b2c2db",
   "metadata": {},
   "source": [
    "# Conformal Predictor functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ced45",
   "metadata": {},
   "source": [
    "The ConformityPredictor function has a computational complexity of O(otherData ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9d06a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first function: conforityPredictor\n",
    "#the inputs include: currentData = the datapoint you want a conformity measure for\n",
    "#currentLabel = the corresponding label for the current data point\n",
    "#otherData = the other datapoints to compare your current data to\n",
    "#otherLabels = corresponding labels for the other Data points\n",
    "def conformityPredictor(currentData, currentLabel,otherData, otherLabels):\n",
    "    #this uses the same method of calculating distances as the function getDistances\n",
    "    #Note: Maybe the code could have been secioned off to reduce the complexity of the code, this was not done as both functions were already complete\n",
    "    distances = np.zeros((otherData.shape[0],2))\n",
    "    for i in range(otherData.shape[0]):\n",
    "        #calculate the distance based on the number of dimensions (e.g. iris has 4 so the loop will run 4 times)\n",
    "        for j in range(currentData.shape[0]):\n",
    "            #find the square of each differnce\n",
    "            distances[i][0] += (currentData[j]-otherData[i][j])**2\n",
    "        #root the value to find the distance\n",
    "        distances[i] = np.array([math.sqrt(distances[i][0]),i])\n",
    "    #vars to hold the value of the minimum distance to the value\n",
    "    distanceToSameClass = np.inf\n",
    "    distanceToDifClass = np.inf\n",
    "    \n",
    "    #go through all of the distances and select the smallest distance\n",
    "    for i in range(otherData.shape[0]):\n",
    "        if otherLabels[i] == currentLabel and distances[i][0] < distanceToSameClass:\n",
    "            distanceToSameClass = distances[i][0]\n",
    "        elif distances[i][0] < distanceToDifClass:\n",
    "            distanceToDifClass = distances[i][0]\n",
    "    #following if statements avoid the problem of dividing by 0 and ensure that the corret value is returned\n",
    "    if distanceToSameClass == 0:\n",
    "        return 0\n",
    "    elif distanceToDifClass == 0:\n",
    "        return np.inf\n",
    "    else:\n",
    "        return distanceToDifClass/distanceToSameClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094cfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get p value for a point\n",
    "#inputs: testDataPoint = the datapoint you want a p value for\n",
    "#currentLabel = the corresponding label for the current data point\n",
    "#otherData = the other datapoints to compare your current data to\n",
    "#otherLabels = corresponding labels for the other Data points\n",
    "def pValues(testDatapoint, predictedLabel,trainingData, trainingLabels):\n",
    "    fullDataArray = np.concatenate(([testDatapoint], trainingData))\n",
    "    fullLabelArray = np.concatenate(([predictedLabel], trainingLabels))\n",
    "    conformityScores = np.zeros(fullLabelArray.shape[0])\n",
    "    #obtain an array of all conformity scores\n",
    "    #get the first value\n",
    "    currentData = fullDataArray[0]\n",
    "    currentLabel = fullLabelArray[0]\n",
    "    otherData = fullDataArray[:][1:]\n",
    "    otherLabels = fullLabelArray[:][1:]\n",
    "    conformityScores[0] = conformityPredictor(currentData,currentLabel, otherData, otherLabels)\n",
    "    #get the other values\n",
    "    for i in range(fullDataArray.shape[0])[1:]:\n",
    "        currentData = fullDataArray[i]\n",
    "        currentLabel = fullLabelArray[i]\n",
    "        otherData = np.concatenate(([fullDataArray[i-1]],fullDataArray[:][i+2:]))\n",
    "        otherLabels = np.concatenate(([fullLabelArray[i-1]],fullLabelArray[:][i+2:]))\n",
    "        conformityScores[i] = conformityPredictor(currentData,currentLabel, otherData, otherLabels)\n",
    "    #set the var to hold the pValue \n",
    "    pValue = 0\n",
    "    #loop through all of the conformity scores and if the conformity score is higher or equal to another then add 1 to pVal\n",
    "    #use of >= allows for Modified competition ranking, compared to Standard competition ranking which is achieved by >\n",
    "    for i in range(conformityScores.shape[0])[1:]:\n",
    "        if conformityScores[0] >= conformityScores[i]:\n",
    "            pValue += 1\n",
    "    pValue = pValue/conformityScores.shape[0]\n",
    "    return pValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e82eb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get p values for an array of points\n",
    "#inputs: testDataPoint = the datapoints you want a p values for\n",
    "#currentLabel = the corresponding labels for the test datapoints\n",
    "#otherData = the other datapoints to compare your current data to\n",
    "#otherLabels = corresponding labels for the other Data points\n",
    "def pValuesForGroup(testDatapoints, predictedLabels,trainingData, trainingLabels):\n",
    "    pValuesArray = np.zeros(testDatapoints.shape[0])\n",
    "    #loop through all points\n",
    "    for i in range(testDatapoints.shape[0]):\n",
    "        #fetch all of the p values for each test data point\n",
    "        pValuesArray[i] = pValues(testDatapoints[i],predictedLabels[i], trainingData, trainingLabels)\n",
    "    return pValuesArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f329872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the false p value\n",
    "#inputs: pValues = an array of p values\n",
    "#predicted labels = the predicted labels which correspond to the p values\n",
    "#true labels = the true labels which correspond to the p values\n",
    "def falsePValues(pValues, predictedLabels, trueLabels):\n",
    "    #loop through all values\n",
    "    falsePredictions = 0\n",
    "    sumPVals = 0\n",
    "    #loop through all p values\n",
    "    for i in range(pValues.shape[0]):\n",
    "        #if the predicted label was false then sum add up the the p values, then divide by the number of false predictions\n",
    "        if predictedLabels[i] == trueLabels[i]:\n",
    "            falsePredictions += 1\n",
    "            sumPVals += pValues[i]\n",
    "    return sumPVals/falsePredictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6af05",
   "metadata": {},
   "source": [
    "# Tests on iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc02418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "#alt method\n",
    "#irisData = np.genfromtxt(\"iris_data.txt\")\n",
    "#irisTarget = np.genfromtxt(\"iris_target.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a7c39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitData = dataSplit(iris['data'],iris['target'],0.25,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc613c2",
   "metadata": {},
   "source": [
    "# 1NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24779638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 0. 1. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0. 0. 2. 1. 0. 0. 0. 2. 2. 2. 0.\n",
      " 1. 0. 1. 1. 1. 2. 1. 1. 2. 2. 2. 0. 2.]\n"
     ]
    }
   ],
   "source": [
    "predictedLabels = applyToTestSet(splitData[0],splitData[2],splitData[1], 1)\n",
    "print(predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daa6abd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 0. 1. 0. 1. 1. 1. 0. 1. 1. 2. 1. 0. 0. 2. 1. 0. 0. 0. 2. 2. 2. 0.\n",
      " 1. 0. 1. 1. 1. 2. 1. 1. 2. 2. 2. 0. 2.]\n",
      "0.972972972972973\n"
     ]
    }
   ],
   "source": [
    "print(splitData[3])\n",
    "print(predictionAccuracy(predictedLabels,splitData[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c849c",
   "metadata": {},
   "source": [
    "# Average False P Value for 1NN applied to iris.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a09a166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87719298 0.26315789 0.92982456 0.81578947 0.59649123 0.40350877\n",
      " 0.11403509 0.5        0.51754386 0.16666667 0.87719298 0.02631579\n",
      " 0.59649123 0.11403509 0.73684211 0.83333333 0.28947368 0.6754386\n",
      " 0.10526316 0.16666667 0.20175439 0.94736842 0.61403509 0.51754386\n",
      " 0.34210526 0.22807018 0.37719298 0.60526316 0.22807018 0.78070175\n",
      " 0.11403509 0.8245614  0.60526316 0.16666667 0.77192982 0.59649123\n",
      " 0.19298246]\n"
     ]
    }
   ],
   "source": [
    "arrayOfPValues = pValuesForGroup(splitData[1], predictedLabels, splitData[0], splitData[2])\n",
    "print(arrayOfPValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532c979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48903508771929816\n"
     ]
    }
   ],
   "source": [
    "print(falsePValues(arrayOfPValues, predictedLabels, splitData[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c52763",
   "metadata": {},
   "source": [
    "# 3NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0265007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 0. 1. 0. 1. 2. 1. 0. 1. 1. 2. 1. 0. 0. 2. 1. 0. 0. 0. 2. 2. 2. 0.\n",
      " 1. 0. 1. 1. 1. 2. 1. 1. 2. 2. 2. 0. 2.]\n"
     ]
    }
   ],
   "source": [
    "predictedLabels = applyToTestSet(splitData[0],splitData[2],splitData[1], 3)\n",
    "print(predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "290d76eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 0. 1. 0. 1. 1. 1. 0. 1. 1. 2. 1. 0. 0. 2. 1. 0. 0. 0. 2. 2. 2. 0.\n",
      " 1. 0. 1. 1. 1. 2. 1. 1. 2. 2. 2. 0. 2.]\n",
      "0.972972972972973\n"
     ]
    }
   ],
   "source": [
    "print(splitData[3])\n",
    "print(predictionAccuracy(predictedLabels,splitData[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b90fc",
   "metadata": {},
   "source": [
    "# Tests on ionosphere dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "300accfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the ionosphere\n",
    "X = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\")\n",
    "#seperate the labels from data\n",
    "ionosphereData = X[:,0:33]\n",
    "ionosphereLabels = X[:,34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "024433a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitData = dataSplit(ionosphereData,ionosphereLabels,0.25,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e4eea",
   "metadata": {},
   "source": [
    "# 1NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd196e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1. -1.  1.  1.  1.  1. -1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      " -1.  1.  1. -1.  1. -1.  1. -1.  1. -1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "predictedLabels = applyToTestSet(splitData[0],splitData[2],splitData[1], 1)\n",
    "print(predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f4d2976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1. -1.  1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1.  1.  1.\n",
      " -1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1.  1. -1. -1. -1.\n",
      " -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1.  1.  1.]\n",
      "0.8275862068965517\n"
     ]
    }
   ],
   "source": [
    "print(splitData[3])\n",
    "print(predictionAccuracy(predictedLabels,splitData[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8e89b",
   "metadata": {},
   "source": [
    "# Average False P Value for 1NN applied to ionosphere.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayOfPValues = pValuesForGroup(splitData[1], predictedLabels, splitData[0], splitData[2])\n",
    "print(arrayOfPValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(falsePValues(arrayOfPValues, predictedLabels, splitData[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31519a2",
   "metadata": {},
   "source": [
    "# 3NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLabels = applyToTestSet(splitData[0],splitData[2],splitData[1], 3)\n",
    "print(predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(splitData[3])\n",
    "print(predictionAccuracy(predictedLabels,splitData[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
